{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "-Pbf2NztZ-xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TYVoWQABPnR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7r7d-avLZSd",
        "outputId": "79615ce7-8912-4acc-a2ea-dd04f27e4faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemma = lemmatizer.lemmatize\n",
        "my_stop_words = [lemma(t) for t in stopwords.words('english')]\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    table = text.maketrans({key: None for key in string.punctuation})\n",
        "    text = text.translate(table)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    no_punct = remove_punctuation(text)\n",
        "    stems = [lemma(t) for t in word_tokenize(no_punct) if not t in my_stop_words ]\n",
        "    return stems\n",
        "\n",
        "def naive_terms(texts, n=3):\n",
        "    print(\"Term finding started.\")\n",
        "    vectorizer = CountVectorizer(tokenizer = tokenize, strip_accents = 'ascii', ngram_range = (1,n))#, stop_words=my_stop_words)\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    terms = vectorizer.inverse_transform(X)\n",
        "    print(\"Term finding finished.\")\n",
        "    return terms"
      ],
      "metadata": {
        "id": "K5ewam2PEjlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('skills.csv')\n",
        "skills_texts = data['ALL'].to_list()"
      ],
      "metadata": {
        "id": "PzlUzfqRBuVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terms = naive_terms(skills_texts)\n",
        "terms = list(x for c in terms for x in c)\n",
        "my_counter = Counter()\n",
        "my_counter.update(terms)\n",
        "# Code to sort based in count value\n",
        "#sorted_count = {k: v for k, v in sorted(my_counter.items(), key=lambda item: item[1], reverse=True)}\n",
        "most_common = [ x[0] for x in my_counter.most_common(800) ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHfMOmzsEinX",
        "outputId": "5544a364-dc2f-4913-bcb0-42f572571a0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term finding started.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term finding finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "most_common"
      ],
      "metadata": {
        "id": "w9XGEg6DWBD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from transformers import AutoTokenizer, T5EncoderModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "def embedd_bert(text):\n",
        "  st_model = 't5-large'\n",
        "  batch_size = 32\n",
        "  tokenizer = AutoTokenizer.from_pretrained(st_model)\n",
        "  model = T5EncoderModel.from_pretrained(st_model).to(device)\n",
        "  word_embeddings = []\n",
        "  for i in tqdm(range(0,len(text),batch_size), desc=\"Embedding for \"+\" Size: \"+str(len(text))):\n",
        "    encoded_input = tokenizer(text[i:i+batch_size], return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "      model_output = model(**encoded_input)\n",
        "      word_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "      word_embedding = F.normalize(word_embedding, p=2, dim=1)\n",
        "      word_embeddings = word_embeddings + word_embedding.tolist()\n",
        "  return word_embeddings"
      ],
      "metadata": {
        "id": "naT9D6i0Z69H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = embedd_bert(most_common)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THpHbCYyaS4W",
        "outputId": "f6648c35-1072-410e-de95-619eddab9c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding for  Size: 800: 100%|██████████| 25/25 [01:21<00:00,  3.26s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_embeddings = pd.DataFrame({'Keyword':most_common, 'embedding':word_embeddings})"
      ],
      "metadata": {
        "id": "ijggjpBrd6QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.cluster import KMeansClusterer\n",
        "import nltk\n",
        "\n",
        "def clustering(data,NUM_CLUSTERS = 15):\n",
        "\n",
        "    sentences = data['Keyword']\n",
        "\n",
        "    X = np.array(data['embedding'].tolist())\n",
        "\n",
        "    kclusterer = KMeansClusterer(\n",
        "        NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,\n",
        "        repeats=25,avoid_empty_clusters=True)\n",
        "\n",
        "    assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
        "\n",
        "    data['cluster'] = pd.Series(assigned_clusters, index=data.index)\n",
        "    data['centroid'] = data['cluster'].apply(lambda x: kclusterer.means()[x])\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "VLCLXwZUe9Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_clusters = clustering(keyword_embeddings, 2)"
      ],
      "metadata": {
        "id": "CDJAJnb8fGR0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}